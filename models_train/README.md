Model training 
===

Subfolders for the 1-D and 2-D settings. In each setting, the three advanced models (XGB, RNN-oFF and RNN-oREC) have respective subdirectories, each containing the same or similar files. 

- `alvis.sh` : a bash-file used to instruct the SNIC computational cluster on how to run files.
- `desc.txt` : description of the model, may contain notes around tried configurations
- `best_config.yaml` : configurations and hyperparameters used in the final models that were presented in the thesis. 
- `train_model.py` : training script. For the RNN models, the function model_specification builds the network using the Keras functional API and returns the full model, and for the XGB model, the model specification is from the XGBForecaster class found in models/xgbforecaster.py. All training during the work on the thesis was done in connection to Weights & Biases (wandb.ai). The team g_and_f is assigned, as this is the workspace on the wandb website that was used in the thesis. It is simple and free of charge to start an account and workspace for private individuals, but last we checked companies have to pay. Wandb is a great tool for keeping track of models, and is highly recommended to work with for developing models. One advantage is that for each run, wandb can save the best model from the best epoch in the training process in terms of validation loss. This was done for the RNN models, and the models are saved in `.h5`-format. All trained models used a random seed to ensure replicability of the experiments. Saving XGB models differs as the XGB implementation in this project is a structured collection of individual XGB models. These models were then in stead saved after the training session, when it was deemed that a suitable configuration had been found (see `models/xgbforecaster.py` for details on how XGB models are saved in this project).
- `sweep.py` : wandb has a function called a "sweep", which essentially is a hyperparameter search. This was used to perform big hyperparameter searches. This file integrates this functionality with python. 
- `sweep_config.yaml` : declares the searchspace for the sweep. Only prevalent in the 1-D subfolders, as the architecture and hyperparamters were optimized for the 1-D setting (see report). As can be seen in the `train_model.py` scripts, they were adaptive to be run as sweeps or as single training sessions. 
